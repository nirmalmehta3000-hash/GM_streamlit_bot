{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_B0Q-MuiNPh",
        "outputId": "e312c95e-e7ff-44cc-835e-655df11e0d25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1564, which is longer than the specified 1000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to create vector database...\n",
            "Knowledgebase created and saved!\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Streamlit app available at: NgrokTunnel: \"https://52744c02b0d6.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.185.138.5:8501\u001b[0m\n",
            "\u001b[0m\n",
            "/content/app.py:73: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
            "2025-09-19 07:03:03.518381: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758265383.547771   17921 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758265383.556468   17921 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758265383.578254   17921 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758265383.578300   17921 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758265383.578306   17921 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758265383.578310   17921 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "/content/app.py:91: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
            "/content/app.py:92: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
            "  stuff_chain = StuffDocumentsChain(\n",
            "/content/app.py:97: LangChainDeprecationWarning: This class is deprecated. Use the `create_retrieval_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/\n",
            "  return RetrievalQA(\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install streamlit langchain langchain-community faiss-cpu sentence-transformers langchain-google-genai pyngrok python-dotenv openpyxl -q\n",
        "\n",
        "# Install required libraries for Grok (if you intend to use it, otherwise this line can be removed)\n",
        "!pip install langchain_xai -q\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "from pyngrok import ngrok\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain # Import StuffDocumentsChain\n",
        "\n",
        "\n",
        "# Import LLM models (if you intend to use Grok, otherwise remove ChatXAI import)\n",
        "try:\n",
        "    from langchain_xai import ChatXAI\n",
        "except ImportError:\n",
        "    ChatXAI = None\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# CONFIG\n",
        "# ============================================\n",
        "# Replace with your actual API keys or load them from environment variables/secrets\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBNrZz4kTLxxNo5lTW3c5aZ4KJRc9RwRIY\"\n",
        "os.environ[\"GROK_API_KEY\"] = \"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\" # Replace with your ngrok token\n",
        "\n",
        "VECTOR_DB_PATH = \"vectorstore.faiss\"\n",
        "DATASET_PATH = \"dataset.xlsx\" # Make sure this file is uploaded in /content\n",
        "CHAT_HISTORY_DIR = \"chat_history\"\n",
        "CHAT_HISTORY_FILE = os.path.join(CHAT_HISTORY_DIR, \"chat_history.csv\")\n",
        "\n",
        "\n",
        "# Expert system instruction (shared)\n",
        "SYSTEM_INSTRUCTION = (\n",
        "    \"You are a friendly, insightful customer service assistant for www.gerrysonmehta.com. \"\n",
        "    \"Your expertise is helping aspiring data analysts, students, and professionals with career advice, project ideas, interview prep, portfolio building, and time management. \"\n",
        "    \"Respond conversationally, with empathy, encouragement, and actionable steps; like a human expert mentor. \"\n",
        "    \"Always personalize your guidance, use clear language and be honest when uncertain. Never limit help to code debugging. \"\n",
        "    \"Refer to Gerryson Mehta's philosophy and provide motivation as needed.\"\n",
        ")\n",
        "\n",
        "# Prompt template\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Use the following pieces of context to answer the user's question.\n",
        "Respond conversationally, as a human expert coach.\n",
        "If you don't know the answer, just say so honestly and avoid guessing.\n",
        "----------------\n",
        "{context}\n",
        "Question: {question}\n",
        "Expert Answer:\n",
        "\"\"\"\n",
        "\n",
        "# ============================================\n",
        "# FUNCTIONS\n",
        "# ============================================\n",
        "def create_vector_db():\n",
        "    if not os.path.exists(DATASET_PATH):\n",
        "        print(f\"Dataset not found at {DATASET_PATH}. Please upload it.\")\n",
        "        return None # Return None to indicate failure\n",
        "    try:\n",
        "        df = pd.read_excel(DATASET_PATH) # Read Excel file instead of CSV\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading dataset file: {e}\")\n",
        "        return None\n",
        "\n",
        "    documents = [Document(page_content=f\"Q: {row['prompt']}\\nA: {row['response']}\") for _, row in df.iterrows()]\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "    vectorstore.save_local(VECTOR_DB_PATH)\n",
        "    print(\"Knowledgebase created and saved!\")\n",
        "    return vectorstore # Return the created vectorstore\n",
        "\n",
        "def get_vectorstore():\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    if not os.path.exists(VECTOR_DB_PATH):\n",
        "        print(\"Knowledgebase not found. Creating from dataset...\")\n",
        "        vectorstore = create_vector_db()\n",
        "        if vectorstore is None:\n",
        "            return None # Creation failed\n",
        "        return vectorstore\n",
        "    return FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "def get_qa_chain(llm):\n",
        "    vectorstore = get_vectorstore()\n",
        "    if vectorstore is None:\n",
        "        st.error(\"Could not load or create knowledgebase.\")\n",
        "        return None\n",
        "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
        "    prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
        "\n",
        "    # Explicitly create LLMChain and StuffDocumentsChain\n",
        "    llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    stuff_chain = StuffDocumentsChain(\n",
        "        llm_chain=llm_chain,\n",
        "        document_variable_name=\"context\" # Ensure this matches the prompt template variable\n",
        "    )\n",
        "\n",
        "    return RetrievalQA(\n",
        "        retriever=retriever,\n",
        "        combine_documents_chain=stuff_chain,\n",
        "        return_source_documents=False # Or True if you want to return source documents\n",
        "    )\n",
        "\n",
        "def determine_llm(question: str):\n",
        "    # Simple rule: if question mentions \"Grok\" or \"xAI\", use Grok; else use Gemini\n",
        "    # You can improve routing logic based on keywords, question intent, etc.\n",
        "    grok_keywords = [\"grok\", \"xai\", \"x-ai\", \"grok model\", \"grok chat\"]\n",
        "    if any(word in question.lower() for word in grok_keywords) and ChatXAI: # Check if ChatXAI was successfully imported\n",
        "        return \"grok\"\n",
        "    return \"gemini\"\n",
        "\n",
        "def save_chat_entry_to_csv(timestamp, name, email, sender, message):\n",
        "    \"\"\"Saves a single chat entry to a CSV file.\"\"\"\n",
        "    if not os.path.exists(CHAT_HISTORY_DIR):\n",
        "        os.makedirs(CHAT_HISTORY_DIR)\n",
        "\n",
        "    # Check if the file exists to write header only once\n",
        "    file_exists = os.path.exists(CHAT_HISTORY_FILE)\n",
        "\n",
        "    with open(CHAT_HISTORY_FILE, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if not file_exists:\n",
        "            writer.writerow(['Timestamp', 'Name', 'Email', 'Sender', 'Message'])\n",
        "        writer.writerow([timestamp, name, email, sender, message])\n",
        "    print(\"Chat entry saved to CSV.\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# STREAMLIT APP DEFINITION\n",
        "# ============================================\n",
        "def run_app():\n",
        "    st.set_page_config(page_title=\"Gerryson Mehta Multi-LLM Chatbot\", page_icon=\"\")\n",
        "    st.title(\"Gerryson Mehta's Chatbot \")\n",
        "    st.write(\"Powered by Google Gemini and Grok AI\")\n",
        "\n",
        "    # Remove the button to create knowledgebase\n",
        "    # if st.button(\"Create Knowledgebase from CSV\"):\n",
        "    #     with st.spinner(\"Building knowledgebase...\"):\n",
        "    #         create_vector_db()\n",
        "\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        st.session_state.chat_history = []\n",
        "    if \"user_info_collected\" not in st.session_state:\n",
        "        st.session_state.user_info_collected = False\n",
        "    if \"user_name\" not in st.session_state:\n",
        "        st.session_state.user_name = \"\"\n",
        "    if \"user_email\" not in st.session_state:\n",
        "        st.session_state.user_email = \"\"\n",
        "    if \"session_timestamp\" not in st.session_state:\n",
        "        st.session_state.session_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "\n",
        "    # Collect user info if not already collected\n",
        "    if not st.session_state.user_info_collected:\n",
        "        with st.form(\"user_info_form\"):\n",
        "            st.write(\"Please provide your name and email to start the chat.\")\n",
        "            name = st.text_input(\"Name\")\n",
        "            email = st.text_input(\"Email\")\n",
        "            submit_button = st.form_submit_button(\"Start Chat\")\n",
        "\n",
        "            if submit_button:\n",
        "                st.session_state.user_name = name\n",
        "                st.session_state.user_email = email\n",
        "                st.session_state.user_info_collected = True\n",
        "                st.rerun() # Rerun to hide the input fields\n",
        "\n",
        "    if st.session_state.user_info_collected:\n",
        "        st.write(f\"Welcome, {st.session_state.user_name}!\")\n",
        "        question = st.chat_input(\"Ask your career or customer service question here:\")\n",
        "\n",
        "        if question:\n",
        "            llm_choice = determine_llm(question)\n",
        "\n",
        "            llm = None # Initialize llm to None\n",
        "            if llm_choice == \"gemini\":\n",
        "                try:\n",
        "                    llm = GoogleGenerativeAI(\n",
        "                        model=\"gemini-1.5-flash-latest\",\n",
        "                        temperature=0.3,\n",
        "                        # system_instructions=SYSTEM_INSTRUCTION # Removed as it caused a warning\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error initializing Gemini model: {e}.\")\n",
        "            elif llm_choice == \"grok\" and ChatXAI:\n",
        "                 try:\n",
        "                    llm = ChatXAI(\n",
        "                        model=\"grok-4\",\n",
        "                        temperature=0.3,\n",
        "                        xai_api_key=os.environ.get(\"GROK_API_KEY\"),\n",
        "                        # system_instructions=SYSTEM_INSTRUCTION # Removed as it caused a warning\n",
        "                    )\n",
        "                 except Exception as e:\n",
        "                     st.error(f\"Error initializing Grok model: {e}. Falling back to Gemini.\")\n",
        "                     llm_choice = \"gemini\" # Fallback to Gemini\n",
        "                     try:\n",
        "                         llm = GoogleGenerativeAI(\n",
        "                            model=\"gemini-1.5-flash-latest\",\n",
        "                            temperature=0.3,\n",
        "                            # system_instructions=SYSTEM_INSTRUCTION # Removed as it caused a warning\n",
        "                        )\n",
        "                     except Exception as gemini_e:\n",
        "                         st.error(f\"Error initializing fallback Gemini model: {gemini_e}.\")\n",
        "                         llm = None # Ensure llm is None if fallback also fails\n",
        "            else: # Fallback to Gemini if Grok is chosen but ChatXAI is not available\n",
        "                if llm_choice == \"grok\":\n",
        "                     st.warning(\"Grok model selected but langchain_xai is not available. Using Gemini instead.\")\n",
        "                llm_choice = \"gemini\"\n",
        "                try:\n",
        "                    llm = GoogleGenerativeAI(\n",
        "                        model=\"gemini-1.5-flash-latest\",\n",
        "                        temperature=0.3,\n",
        "                        # system_instructions=SYSTEM_INSTRUCTION # Removed as it caused a warning\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error initializing fallback Gemini model: {e}.\")\n",
        "                    llm = None # Ensure llm is None if fallback also fails\n",
        "\n",
        "\n",
        "            if llm: # Only proceed if an LLM was successfully initialized (or fell back to Gemini)\n",
        "                qa_chain = get_qa_chain(llm)\n",
        "                if qa_chain:\n",
        "                    with st.spinner(f\"Using {llm_choice.title()} model to answer...\"):\n",
        "                        response = qa_chain.invoke({\"query\": question})\n",
        "                        answer = response.get(\"result\", \"Could not get an answer from the model.\")\n",
        "                else:\n",
        "                     answer = \"Sorry, I could not load the knowledge base to answer your question.\"\n",
        "\n",
        "                st.session_state.chat_history.append((\"user\", question))\n",
        "                st.session_state.chat_history.append((\"assistant\", answer))\n",
        "\n",
        "                # Save chat history to CSV\n",
        "                save_chat_entry_to_csv(\n",
        "                    st.session_state.session_timestamp,\n",
        "                    st.session_state.user_name,\n",
        "                    st.session_state.user_email,\n",
        "                    \"User\",\n",
        "                    question\n",
        "                )\n",
        "                save_chat_entry_to_csv(\n",
        "                    st.session_state.session_timestamp,\n",
        "                    st.session_state.user_name,\n",
        "                    st.session_state.user_email,\n",
        "                    \"Assistant\",\n",
        "                    answer\n",
        "                )\n",
        "\n",
        "\n",
        "            else: # Should not happen with fallback, but as a safeguard\n",
        "                 st.session_state.chat_history.append((\"user\", question))\n",
        "                 st.session_state.chat_history.append((\"assistant\", \"Sorry, I encountered an issue with the language model and cannot answer your question at this time.\"))\n",
        "\n",
        "                 # Attempt to save the user query even if LLM failed\n",
        "                 save_chat_entry_to_csv(\n",
        "                     st.session_state.session_timestamp,\n",
        "                     st.session_state.user_name,\n",
        "                     st.session_state.user_email,\n",
        "                     \"User\",\n",
        "                     question\n",
        "                 )\n",
        "\n",
        "\n",
        "        # Display chat history\n",
        "        for sender, text in st.session_state.chat_history:\n",
        "            with st.chat_message(sender):\n",
        "                st.markdown(text)\n",
        "\n",
        "# ============================================\n",
        "# LAUNCH STREAMLIT APP WITH NGROK\n",
        "# ============================================\n",
        "# Write the Streamlit app to a file\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    # Write the contents of the run_app function to app.py\n",
        "    # We need to reconstruct the app.py content based on the run_app function\n",
        "    # This requires writing the necessary imports and function definitions into app.py\n",
        "    f.write(\"\"\"\n",
        "import streamlit as st\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import csv # Import the csv module\n",
        "\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain # Import StuffDocumentsChain\n",
        "\n",
        "\n",
        "# Import LLM models (if you intend to use Grok)\n",
        "try:\n",
        "    from langchain_xai import ChatXAI\n",
        "except ImportError:\n",
        "    ChatXAI = None # Handle case where langchain_xai is not installed\n",
        "\n",
        "VECTOR_DB_PATH = \"vectorstore.faiss\"\n",
        "DATASET_PATH = \"dataset.xlsx\" # Make sure this file is uploaded in /content\n",
        "CHAT_HISTORY_DIR = \"chat_history\"\n",
        "CHAT_HISTORY_FILE = os.path.join(CHAT_HISTORY_DIR, \"chat_history.csv\")\n",
        "\n",
        "\n",
        "# Expert system instruction (shared)\n",
        "SYSTEM_INSTRUCTION = (\n",
        "    \"You are a friendly, insightful customer service assistant for www.gerrysonmehta.com. \"\n",
        "    \"Your expertise is helping aspiring data analysts, students, and professionals with career advice, project ideas, interview prep, portfolio building and time management. \"\n",
        "    \"Respond conversationally, with empathy, encouragement and actionable steps; like a human expert mentor. \"\n",
        "    \"Always personalize your guidance, use clear language and be honest when uncertain. Never limit help to code debugging. \"\n",
        "    \"Refer to Gerryson Mehta's philosophy and provide motivation as needed.\"\n",
        ")\n",
        "\n",
        "# Prompt template\n",
        "PROMPT_TEMPLATE = \\\"\\\"\\\"\n",
        "Use the following pieces of context to answer the user's question.\n",
        "Respond conversationally, as a human expert coach.\n",
        "If you don't know the answer, just say so honestly and avoid guessing.\n",
        "----------------\n",
        "{context}\n",
        "Question: {question}\n",
        "Expert Answer:\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "\n",
        "def create_vector_db():\n",
        "    if not os.path.exists(DATASET_PATH):\n",
        "        st.error(f\"Dataset not found at {{DATASET_PATH}}. Please upload it.\")\n",
        "        return None # Return None to indicate failure\n",
        "    try:\n",
        "        df = pd.read_excel(DATASET_PATH) # Read Excel file instead of CSV\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error reading dataset file: {{e}}\")\n",
        "        return None\n",
        "\n",
        "    documents = [Document(page_content=f\"Q: {{row['prompt']}}\\\\nA: {{row['response']}}\") for _, row in df.iterrows()]\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "    vectorstore.save_local(VECTOR_DB_PATH)\n",
        "    st.success(\"Knowledgebase created and saved!\")\n",
        "    return vectorstore # Return the created vectorstore\n",
        "\n",
        "\n",
        "def get_vectorstore():\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    if not os.path.exists(VECTOR_DB_PATH):\n",
        "        st.warning(\"Knowledgebase not found. Creating from dataset...\")\n",
        "        vectorstore = create_vector_db()\n",
        "        if vectorstore is None:\n",
        "            return None # Creation failed\n",
        "        return vectorstore\n",
        "    return FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "def get_qa_chain(llm):\n",
        "    vectorstore = get_vectorstore()\n",
        "    if vectorstore is None:\n",
        "        st.error(\"Could not load or create knowledgebase.\")\n",
        "        return None\n",
        "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
        "    prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
        "\n",
        "    # Explicitly create LLMChain and StuffDocumentsChain\n",
        "    llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    stuff_chain = StuffDocumentsChain(\n",
        "        llm_chain=llm_chain,\n",
        "        document_variable_name=\"context\" # Ensure this matches the prompt template variable\n",
        "    )\n",
        "\n",
        "    return RetrievalQA(\n",
        "        retriever=retriever,\n",
        "        combine_documents_chain=stuff_chain,\n",
        "        return_source_documents=False # Or True if you want to return source documents\n",
        "    )\n",
        "\n",
        "def determine_llm(question: str):\n",
        "    grok_keywords = [\"grok\", \"xai\", \"x-ai\", \"grok model\", \"grok chat\"]\n",
        "    if any(word in question.lower() for word in grok_keywords) and ChatXAI: # Check if ChatXAI was successfully imported\n",
        "        return \"grok\"\n",
        "    return \"gemini\"\n",
        "\n",
        "def save_chat_entry_to_csv(timestamp, name, email, sender, message):\n",
        "    \\\"\\\"\\\"Saves a single chat entry to a CSV file.\\\"\\\"\\\"\n",
        "    if not os.path.exists(CHAT_HISTORY_DIR):\n",
        "        os.makedirs(CHAT_HISTORY_DIR)\n",
        "\n",
        "    # Check if the file exists to write header only once\n",
        "    file_exists = os.path.exists(CHAT_HISTORY_FILE)\n",
        "\n",
        "    with open(CHAT_HISTORY_FILE, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if not file_exists:\n",
        "            writer.writerow(['Timestamp', 'Name', 'Email', 'Sender', 'Message'])\n",
        "        writer.writerow([timestamp, name, email, sender, message])\n",
        "    st.write(\"Chat entry saved to CSV.\")\n",
        "\n",
        "\n",
        "def run_app():\n",
        "    st.set_page_config(page_title=\"Gerryson Mehta Multi-LLM Chatbot\", page_icon=\"\")\n",
        "    st.title(\"Gerryson Mehta's Chatbot \")\n",
        "    st.write(\"Powered by Google Gemini and Grok AI\")\n",
        "\n",
        "    # Remove the button to create knowledgebase\n",
        "    # if st.button(\"Create Knowledgebase from CSV\"):\n",
        "    #     with st.spinner(\"Building knowledgebase...\"):\n",
        "    #         create_vector_db()\n",
        "\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        st.session_state.chat_history = []\n",
        "    if \"user_info_collected\" not in st.session_state:\n",
        "        st.session_state.user_info_collected = False\n",
        "    if \"user_name\" not in st.session_state:\n",
        "        st.session_state.user_name = \"\"\n",
        "    if \"user_email\" not in st.session_state:\n",
        "        st.session_state.user_email = \"\"\n",
        "    if \"session_timestamp\" not in st.session_state:\n",
        "        st.session_state.session_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "\n",
        "    # Collect user info if not already collected\n",
        "    if not st.session_state.user_info_collected:\n",
        "        with st.form(\"user_info_form\"):\n",
        "            st.write(\"Please provide your name and email to start the chat.\")\n",
        "            name = st.text_input(\"Name\")\n",
        "            email = st.text_input(\"Email\")\n",
        "            submit_button = st.form_submit_button(\"Start Chat\")\n",
        "\n",
        "            if submit_button:\n",
        "                st.session_state.user_name = name\n",
        "                st.session_state.user_email = email\n",
        "                st.session_state.user_info_collected = True\n",
        "                st.rerun() # Rerun to hide the input fields\n",
        "\n",
        "    if st.session_state.user_info_collected:\n",
        "        st.write(f\"Welcome, {{st.session_state.user_name}}!\")\n",
        "        question = st.chat_input(\"Ask your career or customer service question here:\")\n",
        "\n",
        "        if question:\n",
        "            llm_choice = determine_llm(question)\n",
        "\n",
        "            llm = None # Initialize llm to None\n",
        "            if llm_choice == \"gemini\":\n",
        "                try:\n",
        "                    llm = GoogleGenerativeAI(\n",
        "                        model=\"gemini-1.5-flash-latest\",\n",
        "                        temperature=0.3,\n",
        "                        # system_instructions=SYSTEM_INSTRUCTION # Removed as it caused a warning\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error initializing Gemini model: {{e}}.\")\n",
        "            elif llm_choice == \"grok\" and ChatXAI:\n",
        "                 try:\n",
        "                    llm = ChatXAI(\n",
        "                        model=\"grok-4\",\n",
        "                        temperature=0.3,\n",
        "                        xai_api_key=os.environ.get(\"GROK_API_KEY\"),\n",
        "                        # system_instructions=SYSTEM_INSTRUCTION # Removed as it caused a warning\n",
        "                    )\n",
        "                 except Exception as e:\n",
        "                     st.error(f\"Error initializing Grok model: {{e}}. Falling back to Gemini.\")\n",
        "                     llm_choice = \"gemini\" # Fallback to Gemini\n",
        "                     try:\n",
        "                         llm = GoogleGenerativeAI(\n",
        "                            model=\"gemini-1.5-flash-latest\",\n",
        "                            temperature=0.3,\n",
        "                            # system_instructions=SYSTEM_INSTRUCTION # Removed as it caused a warning\n",
        "                        )\n",
        "                     except Exception as gemini_e:\n",
        "                         st.error(f\"Error initializing fallback Gemini model: {{gemini_e}}.\")\n",
        "                         llm = None # Ensure llm is None if fallback also fails\n",
        "            else: # Fallback to Gemini if Grok is chosen but ChatXAI is not available\n",
        "                if llm_choice == \"grok\":\n",
        "                     st.warning(\"Grok model selected but langchain_xai is not available. Using Gemini instead.\")\n",
        "                llm_choice = \"gemini\"\n",
        "                try:\n",
        "                    llm = GoogleGenerativeAI(\n",
        "                        model=\"gemini-1.5-flash-latest\",\n",
        "                        temperature=0.3,\n",
        "                        # system_instructions=SYSTEM_INSTRUCTION # Removed as it caused a warning\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error initializing fallback Gemini model: {{e}}.\")\n",
        "                    llm = None # Ensure llm is None if fallback also fails\n",
        "\n",
        "\n",
        "            if llm: # Only proceed if an LLM was successfully initialized (or fell back to Gemini)\n",
        "                qa_chain = get_qa_chain(llm)\n",
        "                if qa_chain:\n",
        "                    with st.spinner(f\"Using {{llm_choice.title()}} model to answer...\"):\n",
        "                        response = qa_chain.invoke({\"query\": question})\n",
        "                        answer = response.get(\"result\", \"Could not get an answer from the model.\")\n",
        "                else:\n",
        "                     answer = \"Sorry, I could not load the knowledge base to answer your question.\"\n",
        "\n",
        "                st.session_state.chat_history.append((\"user\", question))\n",
        "                st.session_state.chat_history.append((\"assistant\", answer))\n",
        "\n",
        "                # Save chat history to CSV\n",
        "                save_chat_entry_to_csv(\n",
        "                    st.session_state.session_timestamp,\n",
        "                    st.session_state.user_name,\n",
        "                    st.session_state.user_email,\n",
        "                    \"User\",\n",
        "                    question\n",
        "                )\n",
        "                save_chat_entry_to_csv(\n",
        "                    st.session_state.session_timestamp,\n",
        "                    st.session_state.user_name,\n",
        "                    st.session_state.user_email,\n",
        "                    \"Assistant\",\n",
        "                    answer\n",
        "                )\n",
        "\n",
        "\n",
        "            else: # Should not happen with fallback, but as a safeguard\n",
        "                 st.session_state.chat_history.append((\"user\", question))\n",
        "                 st.session_state.chat_history.append((\"assistant\", \"Sorry, I encountered an issue with the language model and cannot answer your question at this time.\"))\n",
        "\n",
        "                 # Attempt to save the user query even if LLM failed\n",
        "                 save_chat_entry_to_csv(\n",
        "                     st.session_state.session_timestamp,\n",
        "                     st.session_state.user_name,\n",
        "                     st.session_state.user_email,\n",
        "                     \"User\",\n",
        "                     question\n",
        "                 )\n",
        "\n",
        "\n",
        "        # Display chat history\n",
        "        for sender, text in st.session_state.chat_history:\n",
        "            with st.chat_message(sender):\n",
        "                st.markdown(text)\n",
        "\n",
        "run_app()\n",
        "\"\"\")\n",
        "\n",
        "# Create the vector database outside the Streamlit app logic\n",
        "# This needs to happen in the notebook environment before the Streamlit app starts\n",
        "print(\"Attempting to create vector database...\")\n",
        "create_vector_db() # Call the function to create the knowledge base\n",
        "\n",
        "# Setup ngrok only if the vector database was created successfully\n",
        "if os.path.exists(VECTOR_DB_PATH):\n",
        "    try:\n",
        "        # Authenticate ngrok (replace with your actual authtoken)\n",
        "        !ngrok config add-authtoken 32mHdHp8QFewuhR98KlGLyvplHM_5HWMhXu2GGQ2V25no9wfk\n",
        "        # Start ngrok tunnel\n",
        "        public_url = ngrok.connect(8501)\n",
        "        print(f\"Streamlit app available at: {public_url}\")\n",
        "        # Run the Streamlit app\n",
        "        !streamlit run app.py &\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up ngrok or running Streamlit: {e}\")\n",
        "else:\n",
        "    print(\"Vector database creation failed. Streamlit app will not start.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeQtnaH0iqqL",
        "outputId": "1c474b76-eb95-4115-cf88-f850cc8f06ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset not found at dataset.csv. Please upload it.\n"
          ]
        }
      ],
      "source": [
        "create_vector_db()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5KbiQ-JisrF"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_xai -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feL6YRF5iwUe",
        "outputId": "14a106f5-edf4-415c-ecac-e26fef9063f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-19 05:57:05.489 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-19 05:57:05.697 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-09-19 05:57:05.698 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-19 05:57:05.702 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ],
      "source": [
        "def create_vector_db(df: pd.DataFrame = None):\n",
        "    # Removed the check for os.path.exists(DATASET_PATH) and CSV reading\n",
        "    if df is None:\n",
        "         # This function is now intended to be called with a DataFrame\n",
        "         # If called without one, it won't proceed\n",
        "         print(\"create_vector_db called without a DataFrame.\")\n",
        "         return\n",
        "\n",
        "    documents = [Document(page_content=f\"Q: {row['prompt']}\\nA: {row['response']}\") for _, row in df.iterrows()]\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "    vectorstore.save_local(VECTOR_DB_PATH)\n",
        "    # Changed st.success to print since this function is called outside Streamlit\n",
        "    print(\"Knowledgebase created and saved!\")\n",
        "\n",
        "# Now, read the Excel file and then call create_vector_db with the DataFrame\n",
        "try:\n",
        "    df = pd.read_excel(DATASET_PATH)\n",
        "    create_vector_db(df)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Dataset not found at {DATASET_PATH}. Please upload it.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error reading dataset file: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdec47c3"
      },
      "source": [
        "# Task\n",
        "Modify the Streamlit application in the file \"app.py\" to collect the connecting user's name, email, and timestamp, store their chat history, and save this information to a file in my Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45adf671"
      },
      "source": [
        "## Modify streamlit app to collect user data\n",
        "\n",
        "### Subtask:\n",
        "Update the `app.py` file to include input fields for the user to enter their name and email, and capture the timestamp of their interaction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f0785cb"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to modify the `app.py` file to add input fields for name and email, store them in session state, and capture the session start timestamp. This involves writing the updated content of the `run_app` function to the `app.py` file.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}