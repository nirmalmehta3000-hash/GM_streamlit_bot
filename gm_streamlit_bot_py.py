# -*- coding: utf-8 -*-
"""GM_streamlit_bot.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iLR1lfqPSLF0vS5NFr72NL0jz8YOzhaP
"""

# Install dependencies
!pip install streamlit langchain langchain-community faiss-cpu sentence-transformers langchain-google-genai pyngrok python-dotenv openpyxl -q

# Install required libraries for Grok (if you intend to use it, otherwise this line can be removed)
!pip install langchain_xai -q

import os
import pandas as pd
import streamlit as st
from pyngrok import ngrok
from datetime import datetime


# LangChain imports
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_google_genai import GoogleGenerativeAI
from langchain.text_splitter import CharacterTextSplitter
from langchain.docstore.document import Document
from langchain_core.prompts import PromptTemplate
from langchain.chains.llm import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain # Import StuffDocumentsChain


# Import LLM models (if you intend to use Grok, otherwise remove ChatXAI import)
try:
    from langchain_xai import ChatXAI
except ImportError:
    ChatXAI = None


# ============================================
# CONFIG
# ============================================
# Replace with your actual API keys or load them from environment variables/secrets
os.environ["GOOGLE_API_KEY"] = "AIzaSyBNrZz4kTLxxNo5lTW3c5aZ4KJRc9RwRIY"
os.environ["GROK_API_KEY"] = "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" # Replace with your ngrok token

VECTOR_DB_PATH = "vectorstore.faiss"
DATASET_PATH = "dataset.xlsx" # Make sure this file is uploaded in /content
CHAT_HISTORY_DIR = "chat_history"
CHAT_HISTORY_FILE = os.path.join(CHAT_HISTORY_DIR, "chat_history.csv")


# Expert system instruction (shared)
SYSTEM_INSTRUCTION = (
    "You are a friendly, insightful customer service assistant for www.gerrysonmehta.com. "
    "Your expertise is helping aspiring data analysts, students, and professionals with career advice, project ideas, interview prep, portfolio building, and time management. "
    "Respond conversationally, with empathy, encouragement, and actionable steps; like a human expert mentor. "
    "Always personalize your guidance, use clear language and be honest when uncertain. Never limit help to code debugging. "
    "Refer to Gerryson Mehta's philosophy and provide motivation as needed."
)

# Prompt template
PROMPT_TEMPLATE = """
Use the following pieces of context to answer the user's question.
Respond conversationally, as a human expert coach.
If you don't know the answer, just say so honestly and avoid guessing.
----------------
{context}
Question: {question}
Expert Answer:
"""

# ============================================
# FUNCTIONS
# ============================================
def create_vector_db():
    if not os.path.exists(DATASET_PATH):
        print(f"Dataset not found at {DATASET_PATH}. Please upload it.")
        return None # Return None to indicate failure
    try:
        df = pd.read_excel(DATASET_PATH) # Read Excel file instead of CSV
    except Exception as e:
        print(f"Error reading dataset file: {e}")
        return None

    documents = [Document(page_content=f"Q: {row['prompt']}\nA: {row['response']}") for _, row in df.iterrows()]
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    texts = text_splitter.split_documents(documents)
    embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = FAISS.from_documents(texts, embeddings)
    vectorstore.save_local(VECTOR_DB_PATH)
    print("Knowledgebase created and saved!")
    return vectorstore # Return the created vectorstore

def get_vectorstore():
    embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
    if not os.path.exists(VECTOR_DB_PATH):
        print("Knowledgebase not found. Creating from dataset...")
        vectorstore = create_vector_db()
        if vectorstore is None:
            return None # Creation failed
        return vectorstore
    return FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)

def get_qa_chain(llm):
    vectorstore = get_vectorstore()
    if vectorstore is None:
        st.error("Could not load or create knowledgebase.")
        return None
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k":3})
    prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)

    # Explicitly create LLMChain and StuffDocumentsChain
    llm_chain = LLMChain(llm=llm, prompt=prompt)
    stuff_chain = StuffDocumentsChain(
        llm_chain=llm_chain,
        document_variable_name="context" # Ensure this matches the prompt template variable
    )

    return RetrievalQA(
        retriever=retriever,
        combine_documents_chain=stuff_chain,
        return_source_documents=False # Or True if you want to return source documents
    )

def determine_llm(question: str):
    # Simple rule: if question mentions "Grok" or "xAI", use Grok; else use Gemini
    # You can improve routing logic based on keywords, question intent, etc.
    grok_keywords = ["grok", "xai", "x-ai", "grok model", "grok chat"]
    if any(word in question.lower() for word in grok_keywords) and ChatXAI: # Check if ChatXAI was successfully imported
        return "grok"
    return "gemini"

def save_chat_entry_to_csv(timestamp, name, email, sender, message):
    """Saves a single chat entry to a CSV file."""
    if not os.path.exists(CHAT_HISTORY_DIR):
        os.makedirs(CHAT_HISTORY_DIR)

    # Check if the file exists to write header only once
    file_exists = os.path.exists(CHAT_HISTORY_FILE)

    with open(CHAT_HISTORY_FILE, mode='a', newline='') as file:
        writer = csv.writer(file)
        if not file_exists:
            writer.writerow(['Timestamp', 'Name', 'Email', 'Sender', 'Message'])
        writer.writerow([timestamp, name, email, sender, message])
    print("Chat entry saved to CSV.")


# ============================================
# STREAMLIT APP DEFINITION
# ============================================
def run_app():
    st.set_page_config(page_title="Gerryson Mehta Multi-LLM Chatbot", page_icon="")
    st.title("Gerryson Mehta's Chatbot ")
    st.write("Powered by Google Gemini and Grok AI")

    # Remove the button to create knowledgebase
    # if st.button("Create Knowledgebase from CSV"):
    #     with st.spinner("Building knowledgebase..."):
    #         create_vector_db()

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "user_info_collected" not in st.session_state:
        st.session_state.user_info_collected = False
    if "user_name" not in st.session_state:
        st.session_state.user_name = ""
    if "user_email" not in st.session_state:
        st.session_state.user_email = ""
    if "session_timestamp" not in st.session_state:
        st.session_state.session_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")


    # Collect user info if not already collected
    if not st.session_state.user_info_collected:
        with st.form("user_info_form"):
            st.write("Please provide your name and email to start the chat.")
            name = st.text_input("Name")
            email = st.text_input("Email")
            submit_button = st.form_submit_button("Start Chat")

            if submit_button:
                st.session_state.user_name = name
                st.session_state.user_email = email
                st.session_state.user_info_collected = True
                st.rerun() # Rerun to hide the input fields

    if st.session_state.user_info_collected:
        st.write(f"Welcome, {st.session_state.user_name}!")
        question = st.chat_input("Ask your career or customer service question here:")

        if question:
            llm_choice = determine_llm(question)

            llm = None # Initialize llm to None
            if llm_choice == "gemini":
                try:
                    llm = GoogleGenerativeAI(
                        model="gemini-1.5-flash-latest",
                        temperature=0.3,
                        # system_instructions=SYSTEM_INSTRUCTION # Removed as it caused a warning
                    )
                except Exception as e:
                    st.error(f"Error initializing Gemini model: {e}.")
            elif llm_choice == "grok" and ChatXAI:
                 try:
                    llm = ChatXAI(
                        model="grok-4",
                        temperature=0.3,
                        xai_api_key=os.environ.get("GROK_API_KEY"),
                        # system_instructions=SYSTEM_INSTRUCTION # Removed as it caused a warning
                    )
                 except Exception as e:
                     st.error(f"Error initializing Grok model: {e}. Falling back to Gemini.")
                     llm_choice = "gemini" # Fallback to Gemini
                     try:
                         llm = GoogleGenerativeAI(
                            model="gemini-1.5-flash-latest",
                            temperature=0.3,
                            # system_instructions=SYSTEM_INSTRUCTION # Removed as it caused a warning
                        )
                     except Exception as gemini_e:
                         st.error(f"Error initializing fallback Gemini model: {gemini_e}.")
                         llm = None # Ensure llm is None if fallback also fails
            else: # Fallback to Gemini if Grok is chosen but ChatXAI is not available
                if llm_choice == "grok":
                     st.warning("Grok model selected but langchain_xai is not available. Using Gemini instead.")
                llm_choice = "gemini"
                try:
                    llm = GoogleGenerativeAI(
                        model="gemini-1.5-flash-latest",
                        temperature=0.3,
                        # system_instructions=SYSTEM_INSTRUCTION # Removed as it caused a warning
                    )
                except Exception as e:
                    st.error(f"Error initializing fallback Gemini model: {e}.")
                    llm = None # Ensure llm is None if fallback also fails


            if llm: # Only proceed if an LLM was successfully initialized (or fell back to Gemini)
                qa_chain = get_qa_chain(llm)
                if qa_chain:
                    with st.spinner(f"Using {llm_choice.title()} model to answer..."):
                        response = qa_chain.invoke({"query": question})
                        answer = response.get("result", "Could not get an answer from the model.")
                else:
                     answer = "Sorry, I could not load the knowledge base to answer your question."

                st.session_state.chat_history.append(("user", question))
                st.session_state.chat_history.append(("assistant", answer))

                # Save chat history to CSV
                save_chat_entry_to_csv(
                    st.session_state.session_timestamp,
                    st.session_state.user_name,
                    st.session_state.user_email,
                    "User",
                    question
                )
                save_chat_entry_to_csv(
                    st.session_state.session_timestamp,
                    st.session_state.user_name,
                    st.session_state.user_email,
                    "Assistant",
                    answer
                )


            else: # Should not happen with fallback, but as a safeguard
                 st.session_state.chat_history.append(("user", question))
                 st.session_state.chat_history.append(("assistant", "Sorry, I encountered an issue with the language model and cannot answer your question at this time."))

                 # Attempt to save the user query even if LLM failed
                 save_chat_entry_to_csv(
                     st.session_state.session_timestamp,
                     st.session_state.user_name,
                     st.session_state.user_email,
                     "User",
                     question
                 )


        # Display chat history
        for sender, text in st.session_state.chat_history:
            with st.chat_message(sender):
                st.markdown(text)

# ============================================
# LAUNCH STREAMLIT APP WITH NGROK
# ============================================
# Write the Streamlit app to a file
with open("app.py", "w") as f:
    # Write the contents of the run_app function to app.py
    # We need to reconstruct the app.py content based on the run_app function
    # This requires writing the necessary imports and function definitions into app.py
    f.write("""
import streamlit as st
import os
import pandas as pd
from datetime import datetime
import csv # Import the csv module

from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_google_genai import GoogleGenerativeAI
from langchain.text_splitter import CharacterTextSplitter
from langchain.docstore.document import Document
from langchain_core.prompts import PromptTemplate
from langchain.chains.llm import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain # Import StuffDocumentsChain


# Import LLM models (if you intend to use Grok)
try:
    from langchain_xai import ChatXAI
except ImportError:
    ChatXAI = None # Handle case where langchain_xai is not installed

VECTOR_DB_PATH = "vectorstore.faiss"
DATASET_PATH = "dataset.xlsx" # Make sure this file is uploaded in /content
CHAT_HISTORY_DIR = "chat_history"
CHAT_HISTORY_FILE = os.path.join(CHAT_HISTORY_DIR, "chat_history.csv")


# Expert system instruction (shared)
SYSTEM_INSTRUCTION = (
    "You are a friendly, insightful customer service assistant for www.gerrysonmehta.com. "
    "Your expertise is helping aspiring data analysts, students, and professionals with career advice, project ideas, interview prep, portfolio building and time management. "
    "Respond conversationally, with empathy, encouragement and actionable steps; like a human expert mentor. "
    "Always personalize your guidance, use clear language and be honest when uncertain. Never limit help to code debugging. "
    "Refer to Gerryson Mehta's philosophy and provide motivation as needed."
)

# Prompt template
PROMPT_TEMPLATE = \"\"\"
Use the following pieces of context to answer the user's question.
Respond conversationally, as a human expert coach.
If you don't know the answer, just say so honestly and avoid guessing.
----------------
{context}
Question: {question}
Expert Answer:
\"\"\"


def create_vector_db():
    if not os.path.exists(DATASET_PATH):
        st.error(f"Dataset not found at {{DATASET_PATH}}. Please upload it.")
        return None # Return None to indicate failure
    try:
        df = pd.read_excel(DATASET_PATH) # Read Excel file instead of CSV
    except Exception as e:
        st.error(f"Error reading dataset file: {{e}}")
        return None

    documents = [Document(page_content=f"Q: {{row['prompt']}}\\nA: {{row['response']}}") for _, row in df.iterrows()]
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    texts = text_splitter.split_documents(documents)
    embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = FAISS.from_documents(texts, embeddings)
    vectorstore.save_local(VECTOR_DB_PATH)
    st.success("Knowledgebase created and saved!")
    return vectorstore # Return the created vectorstore


def get_vectorstore():
    embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
    if not os.path.exists(VECTOR_DB_PATH):
        st.warning("Knowledgebase not found. Creating from dataset...")
        vectorstore = create_vector_db()
        if vectorstore is None:
            return None # Creation failed
        return vectorstore
    return FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)

def get_qa_chain(llm):
    vectorstore = get_vectorstore()
    if vectorstore is None:
        st.error("Could not load or create knowledgebase.")
        return None
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k":3})
    prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)

    # Explicitly create LLMChain and StuffDocumentsChain
    llm_chain = LLMChain(llm=llm, prompt=prompt)
    stuff_chain = StuffDocumentsChain(
        llm_chain=llm_chain,
        document_variable_name="context" # Ensure this matches the prompt template variable
    )

    return RetrievalQA(
        retriever=retriever,
        combine_documents_chain=stuff_chain,
        return_source_documents=False # Or True if you want to return source documents
    )

def determine_llm(question: str):
    grok_keywords = ["grok", "xai", "x-ai", "grok model", "grok chat"]
    if any(word in question.lower() for word in grok_keywords) and ChatXAI: # Check if ChatXAI was successfully imported
        return "grok"
    return "gemini"

def save_chat_entry_to_csv(timestamp, name, email, sender, message):
    \"\"\"Saves a single chat entry to a CSV file.\"\"\"
    if not os.path.exists(CHAT_HISTORY_DIR):
        os.makedirs(CHAT_HISTORY_DIR)

    # Check if the file exists to write header only once
    file_exists = os.path.exists(CHAT_HISTORY_FILE)

    with open(CHAT_HISTORY_FILE, mode='a', newline='') as file:
        writer = csv.writer(file)
        if not file_exists:
            writer.writerow(['Timestamp', 'Name', 'Email', 'Sender', 'Message'])
        writer.writerow([timestamp, name, email, sender, message])
    st.write("Chat entry saved to CSV.")


def run_app():
    st.set_page_config(page_title="Gerryson Mehta Multi-LLM Chatbot", page_icon="")
    st.title("Gerryson Mehta's Chatbot ")
    st.write("Powered by Google Gemini and Grok AI")

    # Remove the button to create knowledgebase
    # if st.button("Create Knowledgebase from CSV"):
    #     with st.spinner("Building knowledgebase..."):
    #         create_vector_db()

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "user_info_collected" not in st.session_state:
        st.session_state.user_info_collected = False
    if "user_name" not in st.session_state:
        st.session_state.user_name = ""
    if "user_email" not in st.session_state:
        st.session_state.user_email = ""
    if "session_timestamp" not in st.session_state:
        st.session_state.session_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")


    # Collect user info if not already collected
    if not st.session_state.user_info_collected:
        with st.form("user_info_form"):
            st.write("Please provide your name and email to start the chat.")
            name = st.text_input("Name")
            email = st.text_input("Email")
            submit_button = st.form_submit_button("Start Chat")

            if submit_button:
                st.session_state.user_name = name
                st.session_state.user_email = email
                st.session_state.user_info_collected = True
                st.rerun() # Rerun to hide the input fields

    if st.session_state.user_info_collected:
        st.write(f"Welcome, {{st.session_state.user_name}}!")
        question = st.chat_input("Ask your career or customer service question here:")

        if question:
            llm_choice = determine_llm(question)

            llm = None # Initialize llm to None
            if llm_choice == "gemini":
                try:
                    llm = GoogleGenerativeAI(
                        model="gemini-1.5-flash-latest",
                        temperature=0.3,
                        # system_instructions=SYSTEM_INSTRUCTION # Removed as it caused a warning
                    )
                except Exception as e:
                    st.error(f"Error initializing Gemini model: {{e}}.")
            elif llm_choice == "grok" and ChatXAI:
                 try:
                    llm = ChatXAI(
                        model="grok-4",
                        temperature=0.3,
                        xai_api_key=os.environ.get("GROK_API_KEY"),
                        # system_instructions=SYSTEM_INSTRUCTION # Removed as it caused a warning
                    )
                 except Exception as e:
                     st.error(f"Error initializing Grok model: {{e}}. Falling back to Gemini.")
                     llm_choice = "gemini" # Fallback to Gemini
                     try:
                         llm = GoogleGenerativeAI(
                            model="gemini-1.5-flash-latest",
                            temperature=0.3,
                            # system_instructions=SYSTEM_INSTRUCTION # Removed as it caused a warning
                        )
                     except Exception as gemini_e:
                         st.error(f"Error initializing fallback Gemini model: {{gemini_e}}.")
                         llm = None # Ensure llm is None if fallback also fails
            else: # Fallback to Gemini if Grok is chosen but ChatXAI is not available
                if llm_choice == "grok":
                     st.warning("Grok model selected but langchain_xai is not available. Using Gemini instead.")
                llm_choice = "gemini"
                try:
                    llm = GoogleGenerativeAI(
                        model="gemini-1.5-flash-latest",
                        temperature=0.3,
                        # system_instructions=SYSTEM_INSTRUCTION # Removed as it caused a warning
                    )
                except Exception as e:
                    st.error(f"Error initializing fallback Gemini model: {{e}}.")
                    llm = None # Ensure llm is None if fallback also fails


            if llm: # Only proceed if an LLM was successfully initialized (or fell back to Gemini)
                qa_chain = get_qa_chain(llm)
                if qa_chain:
                    with st.spinner(f"Using {{llm_choice.title()}} model to answer..."):
                        response = qa_chain.invoke({"query": question})
                        answer = response.get("result", "Could not get an answer from the model.")
                else:
                     answer = "Sorry, I could not load the knowledge base to answer your question."

                st.session_state.chat_history.append(("user", question))
                st.session_state.chat_history.append(("assistant", answer))

                # Save chat history to CSV
                save_chat_entry_to_csv(
                    st.session_state.session_timestamp,
                    st.session_state.user_name,
                    st.session_state.user_email,
                    "User",
                    question
                )
                save_chat_entry_to_csv(
                    st.session_state.session_timestamp,
                    st.session_state.user_name,
                    st.session_state.user_email,
                    "Assistant",
                    answer
                )


            else: # Should not happen with fallback, but as a safeguard
                 st.session_state.chat_history.append(("user", question))
                 st.session_state.chat_history.append(("assistant", "Sorry, I encountered an issue with the language model and cannot answer your question at this time."))

                 # Attempt to save the user query even if LLM failed
                 save_chat_entry_to_csv(
                     st.session_state.session_timestamp,
                     st.session_state.user_name,
                     st.session_state.user_email,
                     "User",
                     question
                 )


        # Display chat history
        for sender, text in st.session_state.chat_history:
            with st.chat_message(sender):
                st.markdown(text)

run_app()
""")

# Create the vector database outside the Streamlit app logic
# This needs to happen in the notebook environment before the Streamlit app starts
print("Attempting to create vector database...")
create_vector_db() # Call the function to create the knowledge base

# Setup ngrok only if the vector database was created successfully
if os.path.exists(VECTOR_DB_PATH):
    try:
        # Authenticate ngrok (replace with your actual authtoken)
        !ngrok config add-authtoken 32mHdHp8QFewuhR98KlGLyvplHM_5HWMhXu2GGQ2V25no9wfk
        # Start ngrok tunnel
        public_url = ngrok.connect(8501)
        print(f"Streamlit app available at: {public_url}")
        # Run the Streamlit app
        !streamlit run app.py &

    except Exception as e:
        print(f"Error setting up ngrok or running Streamlit: {e}")
else:
    print("Vector database creation failed. Streamlit app will not start.")

create_vector_db()

!pip install langchain_xai -q

def create_vector_db(df: pd.DataFrame = None):
    # Removed the check for os.path.exists(DATASET_PATH) and CSV reading
    if df is None:
         # This function is now intended to be called with a DataFrame
         # If called without one, it won't proceed
         print("create_vector_db called without a DataFrame.")
         return

    documents = [Document(page_content=f"Q: {row['prompt']}\nA: {row['response']}") for _, row in df.iterrows()]
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    texts = text_splitter.split_documents(documents)
    embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = FAISS.from_documents(texts, embeddings)
    vectorstore.save_local(VECTOR_DB_PATH)
    # Changed st.success to print since this function is called outside Streamlit
    print("Knowledgebase created and saved!")

# Now, read the Excel file and then call create_vector_db with the DataFrame
try:
    df = pd.read_excel(DATASET_PATH)
    create_vector_db(df)
except FileNotFoundError:
    print(f"Dataset not found at {DATASET_PATH}. Please upload it.")
except Exception as e:
    print(f"Error reading dataset file: {e}")

"""# Task
Modify the Streamlit application in the file "app.py" to collect the connecting user's name, email, and timestamp, store their chat history, and save this information to a file in my Google Drive.

## Modify streamlit app to collect user data

### Subtask:
Update the `app.py` file to include input fields for the user to enter their name and email, and capture the timestamp of their interaction.

**Reasoning**:
I need to modify the `app.py` file to add input fields for name and email, store them in session state, and capture the session start timestamp. This involves writing the updated content of the `run_app` function to the `app.py` file.
"""